{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Project 2: Modeling, Testing, and Predicting'\nauthor: \"SDS348 Fall 2019\"\ndate: '2019-12-11'\noutput:\n  pdf_document:\n    toc: yes\n  html_document:\n    toc: yes\n    toc_float:\n      collapsed: no\n      smooth_scroll: yes\n---\n\n```{r setup, include=FALSE}\nlibrary(knitr)\nhook_output = knit_hooks$get('output')\nknit_hooks$set(output = function(x, options) {\n  # this hook is used only when the linewidth option is not NULL\n  if (!is.null(n <- options$linewidth)) {\n    x = knitr:::split_lines(x)\n    # any lines wider than n should be wrapped\n    if (any(nchar(x) > n)) x = strwrap(x, width = n)\n    x = paste(x, collapse = '\\n')\n  }\n  hook_output(x, options)\n})\n\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align=\"center\",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)\noptions(tibble.width = 100,width = 100)\nlibrary(tidyverse)\n```\n##Jonathan Choi jjc4292\n\n# Modeling\n\n\n- **0.*\n*I have chosen a data set that contains data on Medical Costs that are billed by insurance to patients. This data set contains 7 variables ranging from sex, age, bmi (body mass index), the number of children they have covered by insurance, whether or not they are a smoker, region, and medical charges from insurance. This data set looks at different factors that may contribute to predicting medical costs billed by their insurance.*\n\n```{R}\nlibrary(tidyverse)\nlibrary(dplyr)\ndata<-read.csv(\"insurance 2.csv\")\ndata<-data%>%na.omit\n\n```\n\n- **1. MANOVA Test*\n\n```{R}\n#MANOVA\nman1<-manova(cbind(bmi,charges)~region, data=data)\nsummary(man1)\n#Univariate ANOVAs\nsummary.aov(man1)\n#Mean Differences \ndata%>%group_by(region)%>%summarize(mean(bmi),mean(charges))\n#post hoc t tests \npairwise.t.test(data$bmi,data$region,p.adj=\"none\")\npairwise.t.test(data$charges, data$region,p.adj=\"none\")\n1-.95^15\nggplot(data, aes(x = age, y = bmi)) +\n geom_point(alpha = .5) + geom_density_2d(h=50) + coord_fixed() + facet_wrap(~region)\ndatA0<-data%>%select(bmi,charges,region)\ncovmats<-datA0%>%group_by(region)%>%do(covs=cov(.[1:2]))\nfor(i in 1:3){print(covmats$covs[i])}\n\n\n```\n*1 MANOVA, 2 ANOVAS, and 12 t tests were conducted for a total of 15 tests. The adjusted alpha level is 0.05/15 = 0.333. The MANOVA test and both ANOVA tests showed a significant difference in mean bmi and mean charges between regions. The results of t tests showed that with an adjusted alpha level, Southeast and Northeast, Northwest and Southeast, Southwest and Northeast, Southwest and Northwest, and Southwest and Southeast show a significant difference in mean bmi. The results of t tests showed that with an adjusted alpha level, Norhwest and Southeast, and Southwest and Southeast show a significant difference in mean charge.  The probability of a type one error is 0.5367088. The assumptions are not likely to have been met because the even though plots look normally distributed, the covariance shows no homogeneity.*\n\n- **2.Randomization Test *\n\n```{R}\ndataA<-data%>%dplyr::select(smoker, bmi)\n\ndataA%>%group_by(smoker) %>% summarise(means= mean(bmi)) %>% summarise(`mean_diff:`=diff(means))\n\t\nrand_dist<-vector()\nfor(i in 1:5000){\nyeet<-data.frame(bmi=sample(dataA$bmi),smoker=dataA$smoker)\nrand_dist[i]<-mean(yeet[yeet$smoker==\"yes\",]$bmi)-\n mean(yeet[yeet$smoker==\"no\",]$bmi)}\n\nmean(rand_dist>0.05665379\t)*2\n\n{hist(rand_dist,main=\"\", ylab=\"\"); abline(v = 0.05665379\t, col = \"red\")}\n\n\n```\n*The null hypothesis is that the mean bmi is the same for being a smoker and non-smoker. The alternative hypothesis is that the mean bmi is different for smokers and non-smokers.The p-value was 0.9144 which is greater than 0.05, which means we fail to reject the null hypothesis. This means that there is sufficient evidence to say that the mean bmi is the same for smoker and non-smokers.*\n\n- **3.Linear Regression Model *\n    \n```{R}\n#Center Means\ndata$bmi_c<-data$bmi-mean(data$bmi)\n#Linear regression model\nfit<-lm(charges ~ bmi_c * region, data=data)\nsummary(fit)\n#Plot\nggplot(data, aes(x=bmi, y=charges,group=region))+geom_point(aes(color=region))+\n geom_smooth(method=\"lm\",formula=y~1,se=F,fullrange=T,aes(color=region))+\ntheme(legend.position=c(.9,.19))+xlab(\"\")\n\n#Assumptions\nlibrary(sandwich)\nlibrary(lmtest)\nbptest(fit)\nresids<-fit$residuals\nfitvals<-fit$fitted.values\nggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')\nggplot()+geom_histogram(aes(resids), bins=20)\nshapiro.test(resids) \n\n#robust standard errors \nsummary(fit)$coef[,1:2]\ncoeftest(fit, vcov = vcovHC(fit))[,1:2]\n\n\n#Main effects \nfit2<-lm(charges ~ bmi_c + region, data=data)\nsummary(fit2)\n```\n*The coefficient estimate of the intercept is 14060.83 which indicates the amount charged when bmi and located in the northeast. The coefficient of bmi_c is 439.26 when all other variables are controlled.This means that if there is 1 unit increase in bmi would then increase amount charged from insurance by 439.26 dollars. The coefficient of regionnorthwest is -1072.01, which means that for 1 increase in bmi it would decrease amount charged by 1072.26 dollars. The coefficient of regionsoutheast is -155.85, which means that for 1 increase in bmi, it would decrease amount charged by 155.85 dollars. The coefficient of regionsouthwest is -1683.67, which means that for 1 increase in bmi, it would decrease the amount charged by 1683.67 dollars. The coefficient of bmi_c:regionnorthwest is -48.96 which means that  for 1 increase in bmi, the slope of regionnorthwest decreases by 48.96. The coefficient of bmi_c:regionsoutheast is -130.85, which means that for 1 increase in bmi, the slope of regionsoutheast decreases by 130.85. The coefficient of bmi_c:southwest is 13.39 which means that for 1 increase in bmi, the slope of regionsouthwest is increased by 13.39. The interaction plot shows the lines to be parallel; thus, there is no significant interaction.  Looking at the ggplot, the homoskedasticity and linearity is not that great. The p-value from the bptest is 2.2x10^-16 which is less than 0.05; thus the homoskedasticity is not met. Based on the graphs, normality looks ok. The Shapiro-Wilk normality test gives a p-value of less than 0.05 which means our distribution is normal. Comparing the uncorrected SE and the robust SEs, the standard errors increased for the intercept, bmi_c, regionnorthwest, regionnsoutheast, regionsouthwest. This means that as the standard errors increased, then the p-values also increased. The model explains 0.04284 of the variation of the outcome.*\n\n- **4.Linear Regression Model (with Interactions)*\n\n```{R}\nsamp_distn<-replicate(5000, {\n boot_dat<-data[sample(nrow(data),replace=TRUE),]\n fit3<-lm(charges ~ bmi_c * region,data=boot_dat)\n coef(fit3)\n})\nsamp_distn%>%t%>%as.data.frame%>%summarize_all(sd)\n\n\n\n```\n*Bootstrapped standard errors are greater than original standard errors but less than robust SEs.*\n\n- **5.Logistic Regression*\n    \n```{R}\n#logistic regression\ndata1<-data%>%mutate(smoker=ifelse(data$smoker==\"yes\",1,0))\nfit4<-glm(smoker~charges+bmi,data=data1,family=binomial(link=\"logit\"))\ncoeftest(fit4)\n#coefficients\nexp(coef(fit4))\n#confusion matrix \ndata$predicted<-predict(fit4, data=data, type = \"response\")\ntable(predict=as.numeric(data$predicted>.5),truth=data$smoker)%>%addmargins\n\n#accuracy\n(220+1029)/1338\n\n#sensitivity \n220/255\n\n#specificity (TNR)\n1029/1064\n\n#PPV\n220/274\n\n#Density plot\n\nodds<-function(p)p/(1-p)\np<-seq(0,1,by=.1)\nlogit<-function(p)log(odds(p))\ndata$logit<- predict(fit4)\nggplot(data) + geom_density(aes(logit, fill=as.factor(smoker)))\n\n\n#ROC plot\nlibrary(plotROC) \nggplot(data,aes(charges,predicted,color=bmi))+geom_line()\nROCplot<-ggplot(data)+geom_roc(aes(d=smoker,m=predicted), n.cuts=0)+\n geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2)\nROCplot\n\n#AUC\ncalc_auc(ROCplot)\n\n##CV\nclass_diag<-function(probs,truth){\n\ntab<-table(factor(probs>.5,levels=c(\"FALSE\",\"TRUE\")),truth)\nacc=sum(diag(tab))/sum(tab)\nsens=tab[2,2]/colSums(tab)[2]\nspec=tab[1,1]/colSums(tab)[1]\nppv=tab[2,2]/rowSums(tab)[2]\n\nif(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1\n\nord<-order(probs, decreasing=TRUE)\nprobs <- probs[ord]; truth <- truth[ord]\n\nTPR=cumsum(truth)/max(1,sum(truth)) \nFPR=cumsum(!truth)/max(1,sum(!truth))\n\ndup<-c(probs[-1]>=probs[-length(probs)], FALSE)\nTPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)\n\nn <- length(TPR)\nauc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )\n\ndata.frame(acc,sens,spec,ppv,auc)\n}\n\nset.seed(1234)\nk=10\n\ndataX<-data[sample(nrow(data)),]\nfolds<-cut(seq(1:nrow(data)),breaks=k,labels=F)\n\ndiags<-NULL\nfor(i in 1:k){\ntrain<-dataX[folds!=i,]\ntest<-dataX[folds==i,]\ntruth<-test$smoker\nfit6<- glm(smoker~charges+bmi,data=train,family=binomial(link=\"logit\"))\nprobs<- predict(fit6, newdata=test, type=\"response\")\ndiags<-rbind(diags,class_diag(probs,truth))}\n\napply(diags,2,mean)\n```\n*For 0 bmi and 0 charge, the odds of being a smoker is 2.6206461. Controlling for bmi, for every 1 unit increase in charges, the odds of being a smoker changes by a factor of 1.0003193. Controlling for charges, for every 1 unit increase in bmi, the odds of being a smoker changes by a factor of 0.7534329. The accuracy is 0.9334828. The sensitivity is 0.8627451. The specificity is 0.9671053. The PPV is 0.8029197. The AUC is 0.9823229, which means that this model is very good at predicting activity. After doing 10-fold CV, the accuracy is 0.9312816, the sensitivity is 0.7948521, the specificity is 0.9663455, the PPV is 0.8569134, and the AUC increased slightly to 0.9824689. There was not much overfitting be4cause there wasn't a big change.*\n\n- **6.LASSO Regression*\n\n```{R}\nlibrary(glmnet)\ny<-as.matrix(data$smoker)\nx<-data%>%dplyr::select(-smoker,-region,-sex,-predicted)%>%mutate_all(scale)%>%as.matrix\ncv<-cv.glmnet(x,y,family=\"binomial\")\nlasso<-glmnet(x,y,family=\"binomial\",lambda=cv$lambda.1se)\ncoef(lasso)\n\nset.seed(1234)\nk=10\n\ndataX<-data[sample(nrow(data)),]\nfolds<-cut(seq(1:nrow(data)),breaks=k,labels=F)\n\ndiags2<-NULL\nfor(i in 1:k){\ntrain2<-dataX[folds!=i,]\ntest2<-dataX[folds==i,]\ntruth2<-test$smoker\nfit7<- glm(smoker~age, bmi, charges,data=train2,family=binomial(link=\"logit\"))\nprobs2<- predict(fit7, newdata=test, type=\"response\")\ndiags2<-rbind(diags2,class_diag(probs2,truth2))}\n\napply(diags2,2,mean)\n\n```\n*The variables that were retained were age, charges, and logit. The AUC was 0.5153859. The specificity was 0.1185185. The accuracy was 0.7350746. The specificity was 0.8906542.  After performing a 10 fold cv, the accuracy from this model was lower than that of my logistic regression in part 5.*\n\n...\n\n\n\n\n\n",
    "created" : 1576095898753.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2580787782",
    "id" : "A6487967",
    "lastKnownWriteTime" : 1576095906,
    "last_content_update" : 1576095906976,
    "path" : "~/Desktop/websiteSDS348/content/Project2.rmd",
    "project_path" : "content/Project2.rmd",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}